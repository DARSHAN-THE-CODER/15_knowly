# -*- coding: utf-8 -*-
"""quizz.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NUoQ3oiz3XPeFM3ogpwp6FlUGbkh1r_m
"""

# !pip install gensim
# !pip install git+https://github.com/boudinfl/pke.git
# !pip install -U nltk
# !pip install -U pywsd
import nltk
nltk.download('stopwords')
nltk.download('popular')

# !pip install transformers
# !pip install torch

from transformers import pipeline

# summarizer = pipeline("summarization")

# f = open("dontcry.txt","r")
# full_text = f.read()


# summary = summarizer(full_text, max_length=50, min_length=10, do_sample=False)[0]['summary_text']

# print(summary)

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import word_tokenize, sent_tokenize

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# full_text = "The Greek historian knew what he was talking about. The Nile River fed Egyptian civilization for hundreds of years. The Longest River the Nile is 4,160 miles long—the world’s longest river. It begins near the equator in Africa and flows north to the Mediterranean Sea. In the south the Nile churns with cataracts. A cataract is a waterfall. Near the sea the Nile branches into a delta. A delta is an area near a river’s mouth where the water deposits fine soil called silt. In the delta, the Nile divides into many streams. The river is called the upper Nile in the south and the lower Nile in the north. For centuries, heavy rains in Ethiopia caused the Nile to flood every summer. The floods deposited rich soil along the Nile’s shores. This soil was fertile, which means it was good for growing crops. Unlike the Tigris and Euphrates, the Nile River flooded at the same time every year, so farmers could predict when to plant their crops."
# print(len(full_text))

def solve(full_text):
    keywords = extract_nouns(full_text) 
    for keyword in keywords:
        filtered_keys.append(keyword)
    sentences = tokenize_sentences(full_text)
    keyword_sentence_mapping = get_sentences_for_keyword(filtered_keys, sentences)
    delete_empty=[]
    for keyword in keyword_sentence_mapping:
        #print(len(keyword_sentence_mapping[keyword]))
        if(len(keyword_sentence_mapping[keyword]) == 0):
            delete_empty.append(keyword)
    for keyword in delete_empty:
        del keyword_sentence_mapping[keyword]
    key_distractor_list = {}

    for keyword in keyword_sentence_mapping:
        #print(keyword_sentence_mapping[keyword])
        wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)
        if wordsense:
            distractors = get_distractors_wordnet(wordsense,keyword)
            if len(distractors) ==0:
                distractors = get_distractors_conceptnet(keyword)
            if len(distractors) != 0:
                key_distractor_list[keyword] = distractors
        else:
            
            distractors = get_distractors_conceptnet(keyword)
            if len(distractors) != 0:
                key_distractor_list[keyword] = distractors

    index = 1
    # print ("#############################################################################")
    # print ("NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. ")
    # print ("#############################################################################\n\n")
    l=[]
    for each in key_distractor_list:

        Dict = {}
        sentence = keyword_sentence_mapping[each][0]
        pattern = re.compile(each, re.IGNORECASE)
        output = pattern.sub( " _______ ", sentence)
        Dict['question'] = output
        print ("%s)"%(index),output)
        #answer
        Dict['answer'] = each
        print('Answer :'+each)
        choices = [each.capitalize()] + key_distractor_list[each]
        top4choices = choices[:4]
        random.shuffle(top4choices)
        options = []
        for choice in top4choices:
            options.append(choice)
        Dict['options'] = options
        l.append(Dict)
        optionchoices = ['a','b','c','d']
        for idx,choice in enumerate(top4choices):
            print ("\t",optionchoices[idx],")"," ",choice)
        #print ("\nMore options: ", choices[4:20],"\n\n")
        index = index + 1
    return l



    
def extract_nouns(text):
    # Tokenize the text into words and remove punctuation and stop words
    words = [word.lower() for word in word_tokenize(text) if word.isalpha() and word.lower() not in stopwords.words("english")]
    
    # Lemmatize the words to reduce them to their base form
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word, pos='n') for word in words]
    
    # Use part-of-speech tagging to identify nouns
    tagged_words = nltk.pos_tag(words)
    nouns = [word for word, pos in tagged_words if pos.startswith("N")]
    
    # Return the list of noun keywords
    return nouns

keywords=[]
filtered_keys=[]


print(keywords)
print (filtered_keys)

#!pip install flashtext

from nltk.tokenize import sent_tokenize
from flashtext import KeywordProcessor

def tokenize_sentences(text):
    sentences = [sent_tokenize(text)]
    sentences = [y for x in sentences for y in x]
    # Remove any short sentences less than 20 letters.
    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]
    return sentences

def get_sentences_for_keyword(keywords, sentences):
    keyword_processor = KeywordProcessor()
    keyword_sentences = {}
    for word in keywords:
        keyword_sentences[word] = []
        keyword_processor.add_keyword(word)
    for sentence in sentences:
        keywords_found = keyword_processor.extract_keywords(sentence)
        for key in keywords_found:
            keyword_sentences[key].append(sentence)

    for key in keyword_sentences.keys():
        values = keyword_sentences[key]
        values = sorted(values, key=len, reverse=True)
        keyword_sentences[key] = values
        return keyword_sentences




import requests
import json
import re
import random
from pywsd.similarity import max_similarity
from pywsd.lesk import adapted_lesk
from nltk.corpus import wordnet as wn

# Distractors from Wordnet
def get_distractors_wordnet(syn,word):
    distractors=[]
    word= word.lower()
    orig_word = word
    if len(word.split())>0:
        word = word.replace(" ","_")
    hypernym = syn.hypernyms()
    if len(hypernym) == 0: 
        return distractors
    for item in hypernym[0].hyponyms():
        name = item.lemmas()[0].name()
        #print ("name ",name, " word",orig_word)
        if name == orig_word:
            continue
        name = name.replace("_"," ")
        name = " ".join(w.capitalize() for w in name.split())
        if name is not None and name not in distractors:
            distractors.append(name)
    return distractors

def get_wordsense(sent,word):
    word= word.lower()
    
    if len(word.split())>0:
        word = word.replace(" ","_")
    
    
    synsets = wn.synsets(word,'n')
    if synsets:
        wup = max_similarity(sent, word, 'wup', pos='n')
        adapted_lesk_output =  adapted_lesk(sent, word, pos='n')
        lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))
        return synsets[lowest_index]
    else:
        return None

# Distractors from http://conceptnet.io/
def get_distractors_conceptnet(word):
    word = word.lower()
    original_word= word
    if (len(word.split())>0):
        word = word.replace(" ","_")
    distractor_list = [] 
    url = "http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5"%(word,word)
    obj = requests.get(url).json()

    for edge in obj['edges']:
        link = edge['end']['term'] 

        url2 = "http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10"%(link,link)
        obj2 = requests.get(url2).json()
        for edge in obj2['edges']:
            word2 = edge['start']['label']
            if word2 not in distractor_list and original_word.lower() not in word2.lower():
                distractor_list.append(word2)
                   
    return distractor_list

# questions = solve(full_text)
# print(questions)